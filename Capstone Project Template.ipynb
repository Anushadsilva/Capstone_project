{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "## Overview\n",
    "The purpose of this data engineering capstone project is to give students a chance to combine what they've learned throughout the program. This project will be an important part of learners portfolio that will help to achieve data engineering-related career goals. We could choose to complete the project provided by the Udacity team or define the scope and data ourselves. I took the first approach in building the DW on the data on immigration to the United States provided by Udacity.\n",
    "\n",
    "## Scenario\n",
    "Modernization of corporations' data warehousing infrastructure by improving performance and ease of use for end users, enhancing functionality, decreasing total cost of ownership while making it possible for real-time decision making. In total, our full suite of services includes helping enterprises with data profiling, data standardization, data acquisition, data transformation and integration.A business consulting firm specialized in data warehouse services through assisting the enterprises with navigating their data needs and creating strategic operational solutions that deliver tangible business results.\n",
    "\n",
    ". We aim to model and create a brand new analytics solution on top of the state-of-the-art technolgies available to enable them to unleash insights from data then providing better customer experiences when coming to the US.\n",
    "\n",
    "## Structure of the Project\n",
    "Following the Udacity guide for this project, we structured this documentation with steps below:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "_Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc_\n",
    "\n",
    "### The Scope \n",
    "The main deliverable of our work here will be a data warehouse in the cloud that will support answering questions through analytics tables and dashboards. Additionally, as we developed a general source-of-truth database, the Government of the US could open the solution through a web API so backend web services could query the warehouse for information relating to international visitors.\n",
    "\n",
    "### The Data\n",
    "For this work we have used the immigration, the global temperature and demographics datasets as well as the descriptions contained in the `I94_SAS_Labels_Descriptions.SAS` file.\n",
    "\n",
    "### The Architecture\n",
    "The whole solution is cloud based on top of __Amazon Web Services (AWS)__. First, all the datasets were preprocessed with __Apache Spark__ and stored in a staging area in __AWS S3__ bucket. Then, we loaded those to a __Amazon Redshift__ cluster using an __Apache Airflow__ pipeline that transfer and check the quality of the data to finally provide our customers a data mart for their convenient analysis.\n",
    "\n",
    "![Architecture](images/architecture.png)\n",
    "\n",
    "The main information and questions a user may want to extract from the data mart would be:\n",
    "\n",
    "* Visitors by nationality.\n",
    "* Visitors by origin.\n",
    "* Visitors by airline.\n",
    "* Correlations between destination in the U.S and the source country.\n",
    "* Correlations between destination in the U.S and source climates.\n",
    "* Correlations between immigration by source region, and the source region temperature.\n",
    "* Correlations between visitor demographics, and states visited.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Explore and Assess the Data\n",
    "\n",
    "_To familiarize ourselves with the data provided by Udacity we have done an exhaustive exploratory data analysis ([EDA](https://en.wikipedia.org/wiki/Exploratory_data_analysis)) checking what data would be useful and what preprocessing steps we should take in order to clean, organize and join the various datasets in a meaningful data model._\n",
    "\n",
    "In the following sections we briefly describe the datasets provided and give a summarized idea on the reasons we took into consideration when deciding what data to use.\n",
    "\n",
    "__Immigration Data__\n",
    "\n",
    "For decades, U.S. immigration officers issued the I-94 Form (Arrival/Departure Record) to foreign visitors (e.g., business visitors, tourists and foreign students) who lawfully entered the United States. The I-94 was a small white paper form that a foreign visitor received from cabin crews on arrival flights and from U.S. Customs and Border Protection at the time of entry into the United States. It listed the traveler's immigration category, port of entry, data of entry into the United States, status expiration date and had a unique 11-digit identifying number assigned to it. Its purpose was to record the traveler's lawful admission to the United States.\n",
    "\n",
    "This is the main dataset and there is a file for each month of the year of 2016 available in the directory `../../data/18-83510-I94-Data-2016/` in the [SAS](https://www.sas.com/en_us/home.html) binary database storage format `sas7bdat`. Combined, the 12 datasets have got more than 40 million rows (40.790.529) and 28 columns. For most of the work we used only the month of April of 2016 which has more than three million records (3.096.313)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries needed in this project\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed, ran successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql import SparkSession\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars\",\"saurfang:spark-sas7bdat:3.0.0-s_2.12\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.load('./sas_data')\n",
    "print(\"Completed, ran successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3096313\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5748517.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>QF</td>\n",
       "      <td>9.495387e+10</td>\n",
       "      <td>00011</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5748518.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NV</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>VA</td>\n",
       "      <td>9.495562e+10</td>\n",
       "      <td>00007</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5748519.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495641e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5748520.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495645e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5748521.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495639e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5748522.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20579.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.498180e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5748523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497969e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5748524.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497975e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5748525.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497325e+10</td>\n",
       "      <td>00028</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5748526.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.501355e+10</td>\n",
       "      <td>00002</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode  \\\n",
       "0  5748517.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "1  5748518.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "2  5748519.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "3  5748520.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "4  5748521.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "5  5748522.0  2016.0     4.0   245.0   464.0     HHW  20574.0      1.0   \n",
       "6  5748523.0  2016.0     4.0   245.0   464.0     HHW  20574.0      1.0   \n",
       "7  5748524.0  2016.0     4.0   245.0   464.0     HHW  20574.0      1.0   \n",
       "8  5748525.0  2016.0     4.0   245.0   464.0     HOU  20574.0      1.0   \n",
       "9  5748526.0  2016.0     4.0   245.0   464.0     LOS  20574.0      1.0   \n",
       "\n",
       "  i94addr  depdate   ...     entdepu  matflag  biryear   dtaddto gender  \\\n",
       "0      CA  20582.0   ...        None        M   1976.0  10292016      F   \n",
       "1      NV  20591.0   ...        None        M   1984.0  10292016      F   \n",
       "2      WA  20582.0   ...        None        M   1987.0  10292016      M   \n",
       "3      WA  20588.0   ...        None        M   1987.0  10292016      F   \n",
       "4      WA  20588.0   ...        None        M   1988.0  10292016      M   \n",
       "5      HI  20579.0   ...        None        M   1959.0  10292016      M   \n",
       "6      HI  20586.0   ...        None        M   1950.0  10292016      F   \n",
       "7      HI  20586.0   ...        None        M   1975.0  10292016      F   \n",
       "8      FL  20581.0   ...        None        M   1989.0  10292016      M   \n",
       "9      CA  20581.0   ...        None        M   1990.0  10292016      F   \n",
       "\n",
       "  insnum airline        admnum  fltno visatype  \n",
       "0   None      QF  9.495387e+10  00011       B1  \n",
       "1   None      VA  9.495562e+10  00007       B1  \n",
       "2   None      DL  9.495641e+10  00040       B1  \n",
       "3   None      DL  9.495645e+10  00040       B1  \n",
       "4   None      DL  9.495639e+10  00040       B1  \n",
       "5   None      NZ  9.498180e+10  00010       B2  \n",
       "6   None      NZ  9.497969e+10  00010       B2  \n",
       "7   None      NZ  9.497975e+10  00010       B2  \n",
       "8   None      NZ  9.497325e+10  00028       B2  \n",
       "9   None      NZ  9.501355e+10  00002       B2  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration = spark.read.parquet(\"sas_data\")\n",
    "print(immigration.count())\n",
    "immigration.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cicid=5748517.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='CA', depdate=20582.0, i94bir=40.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1976.0, dtaddto='10292016', gender='F', insnum=None, airline='QF', admnum=94953870030.0, fltno='00011', visatype='B1')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Data Dictionary__: Below is the description for various fields of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| CICID* | ID that uniquely identify one record in the dataset |\n",
    "| I94YR | 4 digit year |\n",
    "| I94MON | Numeric month |\n",
    "| I94CIT | 3 digit code of source city for immigration (Born country) |\n",
    "| I94RES | 3 digit code of source country for immigration (Residence country) |\n",
    "| I94PORT | Port addmitted through |\n",
    "| ARRDATE | Arrival date in the USA |\n",
    "| I94MODE | Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported) |\n",
    "| I94ADDR | State of arrival |\n",
    "| DEPDATE | Departure date |\n",
    "| I94BIR | Age of Respondent in Years |\n",
    "| I94VISA | Visa codes collapsed into three categories: (1 = Business; 2 = Pleasure; 3 = Student) |\n",
    "| COUNT | Used for summary statistics |\n",
    "| DTADFILE | Character Date Field |\n",
    "| VISAPOST | Department of State where where Visa was issued |\n",
    "| OCCUP | Occupation that will be performed in U.S. |\n",
    "| ENTDEPA | Arrival Flag. Whether admitted or paroled into the US |\n",
    "| ENTDEPD | Departure Flag. Whether departed, lost visa, or deceased |\n",
    "| ENTDEPU | Update Flag. Update of visa, either apprehended, overstayed, or updated to PR |\n",
    "| MATFLAG | Match flag |\n",
    "| BIRYEAR | 4 digit year of birth |\n",
    "| DTADDTO | Character date field to when admitted in the US |\n",
    "| GENDER | Gender |\n",
    "| INSNUM | INS number |\n",
    "| AIRLINE | Airline used to arrive in U.S. |\n",
    "| ADMNUM | Admission number, should be unique and not nullable |\n",
    "| FLTNO | Flight number of Airline used to arrive in U.S. |\n",
    "| VISATYPE | Class of admission legally admitting the non-immigrant to temporarily stay in U.S. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The immigration dataset is our fact so that will be at the center of the star schema model of our data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Global Temperature Data__\n",
    "\n",
    "* World Temperature Data: This dataset came from Kaggle found [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "\n",
    "In the original dataset from [Kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data), several files are available but in this capstone project we will be using only the `GlobalLandTemperaturesByCity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperature_fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "world_temperature = pd.read_csv(temperature_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_temperature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Data Dictionary__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| dt | Date in format YYYY-MM-DD |\n",
    "| AverageTemperature | Average temperature of the city in a given date |\n",
    "| City | City Name |\n",
    "| Country | Country Name |\n",
    "| Latitude | Latitude |\n",
    "| Longitude | Longitude |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The immigration dataset only has data of the US National Tourism Office in the year of 2016, the vast majority of the data here seems not to be suitable. We decided to aggregate this dataset by country, averaging the temperatures and use this reduced table to join with `lookup\\I94CIT_I94RES.csv` lookup table (extracted from `I94_SAS_Labels_Descriptions.SAS`) resulting in the COUNTRY dimension of our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "world_temperature = world_temperature.groupby([\"Country\"]).agg({\"AverageTemperature\": \"mean\", \n",
    "                                                                        \"Latitude\": \"first\", \"Longitude\": \"first\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>13.816497</td>\n",
       "      <td>36.17N</td>\n",
       "      <td>69.61E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>15.525828</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>19.17E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>17.763206</td>\n",
       "      <td>36.17N</td>\n",
       "      <td>3.98E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Angola</td>\n",
       "      <td>21.759716</td>\n",
       "      <td>12.05S</td>\n",
       "      <td>13.15E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>16.999216</td>\n",
       "      <td>39.38S</td>\n",
       "      <td>62.43W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country  AverageTemperature Latitude Longitude\n",
       "0  Afghanistan           13.816497   36.17N    69.61E\n",
       "1      Albania           15.525828   40.99N    19.17E\n",
       "2      Algeria           17.763206   36.17N     3.98E\n",
       "3       Angola           21.759716   12.05S    13.15E\n",
       "4    Argentina           16.999216   39.38S    62.43W"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_temperature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Airports Data__\n",
    "\n",
    "`airport-codes.csv` contains the list of all airport codes, the attributes are identified in datapackage description. Some of the columns contain attributes identifying airport locations, other codes (IATA, local if exist) that are relevant to identification of an airport.\n",
    "Original source url is http://ourairports.com/data/airports.csv (stored in archive/data.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airport = pd.read_csv(\"airport-codes_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Data Dictionary__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| ident | Unique identifier |\n",
    "| type | Type of the airport |\n",
    "| name | Airport Name |\n",
    "| elevation_ft | Altitude of the airport |\n",
    "| continent | Continent |\n",
    "| iso_country | ISO code of the country of the airport |\n",
    "| iso_region | ISO code for the region of the airport |\n",
    "| municipality | City where the airport is located |\n",
    "| gps_code | GPS code of the airport |\n",
    "| iata_code | IATA code of the airport |\n",
    "| local_code | Local code of the airport |\n",
    "| coordinates | GPS coordinates of the airport |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We are not using the airport dataset in our model.We will stick to Global temperature data. Airport data did not prove to be a good source of analysis once we were not able to join this to the main table immigration. No valid and consistent key in both tables in order to cross them. None of the codes (ident, gps_code, iata_code or local_code) seemed to match the columns in the immigration fact table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__U.S. City Demographic Data__\n",
    "\n",
    "This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. This data comes from the US Census Bureau's 2015 American Community Survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_cities_demographics = pd.read_csv(\"us-cities-demographics.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Data Dictionary__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| City | Name of the city |\n",
    "| State | US state of the city |\n",
    "| Median Age | The median of the age of the population |\n",
    "| Male Population | Number of the male population |\n",
    "| Female Population | Number of the female population |\n",
    "| Total Population | Number of the total population |\n",
    "| Number of Veterans | Number of veterans living in the city |\n",
    "| Foreign-born | Number of residents of the city that were not born in the city |\n",
    "| Average Household Size | Average size of the houses in the city |\n",
    "| State Code | Code of the state of the city |\n",
    "| Race | Race class |\n",
    "| Count | Number of individual of each race |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The `US Cities Demographics` is the source of the STATE dimension in our data model. We aggregated the dataset by State and pivoted the `Race` and `Count` columns in order to make each different value of Race to be a column. That way we create a complete table of statistics that summarizes the information for every US state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Step 3: Define the Data Model\n",
    "\n",
    "_In this section of the documentation we detail the process of extract, transform and load the data from the various datasets. As me mentioned before, we are using 3 of the 4 data sources provided by the Udacity team: immigration, temperatures and demographics. Also, we extract descriptions from labels descriptions file `I94_SAS_Labels_Descriptions.SAS`_\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "_Map out the conceptual data model and explain why you chose that model_\n",
    "\n",
    "The data model consists of tables immigration, us_cities_demographics, airport_codes, world_temperature, i94cit_res, i94port, i94mode, i94addr, i94visa\n",
    "\n",
    "The immigration dataset is the origin of the center of our model. As this represent the facts of what we want to analyse - U.S visitors from the world -, this was transformed to the fact table IMMIGRATION as represented in the schema below. We gave this data most of the focus during our modeling phase. The immigration dataset is also the data source for the DATE dimension table. We extracted all the distinct values of the columns arrdate and depdate and applied various functions to store in the table a number of attributes of a particular date: day, month, year, week of year and day of week.\n",
    "\n",
    "![Star-Schema](images/star-schema.PNG)\n",
    "\n",
    "The STATE dimension table is the result of the aggregation of the demographics dataset by the State column. Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born were first aggregated by `City` using `first` function, since they are repeated accross the different rows of the same city. Then, we grouped the resulting rows by `State` applying the `sum` function in the numeric columns to make a cosolidated total in each U.S State. We needed to transform the column `Race` in order to make its different values to become different columns. We achieve this by usig the pivot function of the `pyspark` package. As a result we reached to a final structure where we have got the columns (BlackOrAfricanAmerican, White, ForeignBorn, AmericanIndianAndAlaskaNative, HispanicOrLatino, Asian, NumberVeterans, FemalePopulation, MalePopulation, TotalPopulation) for each of the states of the U.S.\n",
    "\n",
    "The COUNTRY dimention completes our star schema model. To get to the structure we see in the figure above we combined the `GlobalLandTemperaturesByCity` with the code-descriptions found in the file `I94_SAS_Labels_Descriptions.SAS` for the columns `i94cit` and `i94res` showed in the image below.\n",
    "Firstly, we extracted the key-value pairs from the `I94_SAS_Labels_Descriptions.SAS` and saved those in csv files in the `lookup` directory. Following we aggregated the temperature dataset by `City` and then by `Country`. Finally, we join the two intermediary results to form the table COUNTRY. \n",
    "\n",
    "![i94cit](images/i94cit.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "_List the steps necessary to pipeline the data into the chosen data model_\n",
    "\n",
    "\n",
    "To accomplish all the tasks related to the preprocessing of the datasets it was developed a number of functions in a package we called `helper.etl`. There you will find different helper functions to load, select, clean, transform and store the resultind datasets in a very convenient way. The open-source framework Apache Spark was the main tool in this journey. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
    "\n",
    "We concentrated all the logic of preprocessing there in order to only represent here the general steps of the ETL. This notebook here is only for document purposes whereas the actual run of the ETL takes place in the Spark in cloud-native big data platform [Amazon EMR](https://aws.amazon.com/emr/?nc1=h_ls) through the execution of the main function of the `etl` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, re\n",
    "import configparser\n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, when, lower, isnull, year, month, dayofmonth, hour, weekofyear, dayofweek, date_format, to_date\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, DoubleType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The AWS key id and password are configured in a configuration file \"dl.cfg\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "# Reads and saves the AWS access key information and saves them in a environment variable\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "# OUTPUT = config['ETL']['OUTPUT_DATA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed, ran successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql import SparkSession\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars\",\"saurfang:spark-sas7bdat:3.0.0-s_2.12\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.load('./sas_data')\n",
    "print(\"Completed, ran successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read US Cities Demo dataset file\n",
    "demographics=spark.read.csv(\"us-cities-demographics.csv\", sep=';', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Schema to verify that all the columns are in \"string\" format\n",
    "demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cast_type(df, cols):\n",
    "    \"\"\"\n",
    "    Convert the types of the columns according to the configuration supplied in the cols dictionary in the format {\"column_name\": type}\n",
    "    \n",
    "    Args:\n",
    "        df (:obj:`SparkDataFrame`): Spark dataframe to be processed. \n",
    "            Represents the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "        cols (:obj:`dict`): Dictionary in the format of {\"column_name\": type} indicating what columns and types they should be converted to\n",
    "    \"\"\"\n",
    "    for k,v in cols.items():\n",
    "        if k in df.columns:\n",
    "            df = df.withColumn(k, df[k].cast(v))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert numeric columns to the proper types: Integer and Double\n",
    "int_cols = ['Count', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born']\n",
    "float_cols = ['Median Age', 'Average Household Size']\n",
    "demographics = cast_type(demographics, dict(zip(int_cols, len(int_cols)*[IntegerType()])))\n",
    "demographics = cast_type(demographics, dict(zip(float_cols, len(float_cols)*[DoubleType()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# demographics.printSchema()\n",
    "    \n",
    "first_agg = {\"Median Age\": \"first\", \"Male Population\": \"first\", \"Female Population\": \"first\", \n",
    "            \"Total Population\": \"first\", \"Number of Veterans\": \"first\", \"Foreign-born\": \"first\", \"Average Household Size\": \"first\"}\n",
    "\n",
    "# First aggregation - City\n",
    "agg_df = demographics.groupby([\"City\", \"State\", \"State Code\"]).agg(first_agg)\n",
    "\n",
    "# Pivot Table to transform values of the column Race to different columns\n",
    "piv_df = demographics.groupBy([\"City\", \"State\", \"State Code\"]).pivot(\"Race\").sum(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(City='Rockville', State='Maryland', State Code='MD', first(Total Population)=66998, first(Female Population)=35793, first(Median Age)=38.1, first(Number of Veterans)=1990, first(Foreign-born)=25047, first(Male Population)=31205, first(Average Household Size)=2.6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(City='Delray Beach', State='Florida', State Code='FL', American Indian and Alaska Native=None, Asian=1696, Black or African-American=21138, Hispanic or Latino=6397, White=40980)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Rename column names removing the spaces to avoid problems when saving to disk (we got errors when trying to save column names with spaces)\n",
    "demographics = agg_df.join(other=piv_df, on=[\"City\", \"State\", \"State Code\"], how=\"inner\")\\\n",
    "    .withColumnRenamed('State Code', 'StateCode')\\\n",
    "    .withColumnRenamed('first(Total Population)', 'TotalPopulation')\\\n",
    "    .withColumnRenamed('first(Female Population)', 'FemalePopulation')\\\n",
    "    .withColumnRenamed('first(Male Population)', 'MalePopulation')\\\n",
    "    .withColumnRenamed('first(Median Age)', 'MedianAge')\\\n",
    "    .withColumnRenamed('first(Number of Veterans)', 'NumberVeterans')\\\n",
    "    .withColumnRenamed('first(Foreign-born)', 'ForeignBorn')\\\n",
    "    .withColumnRenamed('first(Average Household Size)', 'AverageHouseholdSize')\\\n",
    "    .withColumnRenamed('Hispanic or Latino', 'HispanicOrLatino')\\\n",
    "    .withColumnRenamed('Black or African-American', 'BlackOrAfricanAmerican')\\\n",
    "    .withColumnRenamed('American Indian and Alaska Native', 'AmericanIndianAndAlaskaNative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- StateCode: string (nullable = true)\n",
      " |-- TotalPopulation: integer (nullable = true)\n",
      " |-- FemalePopulation: integer (nullable = true)\n",
      " |-- MedianAge: double (nullable = true)\n",
      " |-- NumberVeterans: integer (nullable = true)\n",
      " |-- ForeignBorn: integer (nullable = true)\n",
      " |-- MalePopulation: integer (nullable = true)\n",
      " |-- AverageHouseholdSize: double (nullable = true)\n",
      " |-- AmericanIndianAndAlaskaNative: long (nullable = true)\n",
      " |-- Asian: long (nullable = true)\n",
      " |-- BlackOrAfricanAmerican: long (nullable = true)\n",
      " |-- HispanicOrLatino: long (nullable = true)\n",
      " |-- White: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "numeric_cols = ['TotalPopulation', 'FemalePopulation', 'MedianAge', 'NumberVeterans', 'ForeignBorn', 'MalePopulation', \n",
    "                'AverageHouseholdSize', 'AmericanIndianAndAlaskaNative', 'Asian', 'BlackOrAfricanAmerican', 'HispanicOrLatino', 'White']\n",
    "\n",
    "# Fill the null values with 0\n",
    "demographics = demographics.fillna(0, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- StateCode: string (nullable = true)\n",
      " |-- TotalPopulation: integer (nullable = true)\n",
      " |-- FemalePopulation: integer (nullable = true)\n",
      " |-- MedianAge: double (nullable = false)\n",
      " |-- NumberVeterans: integer (nullable = true)\n",
      " |-- ForeignBorn: integer (nullable = true)\n",
      " |-- MalePopulation: integer (nullable = true)\n",
      " |-- AverageHouseholdSize: double (nullable = false)\n",
      " |-- AmericanIndianAndAlaskaNative: long (nullable = true)\n",
      " |-- Asian: long (nullable = true)\n",
      " |-- BlackOrAfricanAmerican: long (nullable = true)\n",
      " |-- HispanicOrLatino: long (nullable = true)\n",
      " |-- White: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now write (and overwrite) transformed `demographics` dataset onto parquet file\n",
    "# demographics.write.mode('overwrite').parquet(\"s3a://udacitycapstone-us-immigration/us_cities_demographics.parquet\")\n",
    "demographics.write.mode('overwrite').parquet(\"us_cities_demographics.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read i94 immigration dataset\n",
    "immigration=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "int_cols = ['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', \n",
    "        'arrdate', 'i94mode', 'i94bir', 'i94visa', 'count', 'biryear', 'dtadfile', 'depdate']\n",
    "    \n",
    "date_cols = ['arrdate', 'depdate']\n",
    "    \n",
    "high_null = [\"visapost\", \"occup\", \"entdepu\", \"insnum\"]\n",
    "not_useful_cols = [\"count\", \"entdepa\", \"entdepd\", \"matflag\", \"dtaddto\", \"biryear\", \"admnum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert columns read as string/double to integer\n",
    "immigration = cast_type(immigration, dict(zip(int_cols, len(int_cols)*[IntegerType()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The date format string preferred to our work here: YYYY-MM-DD\n",
    "date_format = \"%Y-%m-%d\"\n",
    "convert_sas_udf = udf(lambda x: x if x is None else (timedelta(days=x) + datetime(1960, 1, 1)).strftime(date_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convert_sas_date(df, cols):\n",
    "    \"\"\"\n",
    "    Convert dates in the SAS datatype to a date in a string format YYYY-MM-DD\n",
    "    \n",
    "    Args:\n",
    "        df (:obj:`SparkDataFrame`): Spark dataframe to be processed. \n",
    "            Represents the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "        cols (:obj:`list`): List of columns in the SAS date format to be convert\n",
    "    \"\"\"\n",
    "    for c in [c for c in cols if c in df.columns]:\n",
    "        df = df.withColumn(c, convert_sas_udf(df[c]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert SAS date to a meaningful string date in the format of YYYY-MM-DD\n",
    "immigration = convert_sas_date(immigration, date_cols)\n",
    "    \n",
    "# Drop high null columns and not useful columns\n",
    "immigration = immigration.drop(*high_null)\n",
    "immigration = immigration.drop(*not_useful_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def date_diff(date1, date2):\n",
    "    '''\n",
    "    Calculates the difference in days between two dates\n",
    "    '''\n",
    "    if date2 is None:\n",
    "        return None\n",
    "    else:\n",
    "        a = datetime.strptime(date1, date_format)\n",
    "        b = datetime.strptime(date2, date_format)\n",
    "        delta = b - a\n",
    "        return delta.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "date_diff_udf = udf(date_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a new columns to store the length of the visitor stay in the US\n",
    "immigration = immigration.withColumn('stay', date_diff_udf(immigration.arrdate, immigration.depdate))\n",
    "immigration = cast_type(immigration, {'stay': IntegerType()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- dtadfile: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- stay: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration.write.mode(\"overwrite\").parquet('immigration.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Start processing the I9I94_SAS_Labels_Description.SAS to create master i94 code dimensions:\n",
    "\n",
    "'''\n",
    "/* I94MODE - There are missing values as well as not reported (9) */\n",
    "\t1 = 'Air'\n",
    "\t2 = 'Sea'\n",
    "\t3 = 'Land'\n",
    "\t9 = 'Not reported' ;\n",
    "'''\n",
    "# Create i94mode list\n",
    "i94mode_data =[[1,'Air'],[2,'Sea'],[3,'Land'],[9,'Not reported']]\n",
    "\n",
    "# Convert to spark dataframe\n",
    "i94mode=spark.createDataFrame(i94mode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94mode.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def etl_demographics_data(spark, input_path=\"us-cities-demographics.csv\", output_path=\"out/demographics.parquet\", \n",
    "                         input_format = \"csv\", columns='*',\n",
    "                          load_size = None, partitionBy = [\"State Code\"], header=True, sep=\";\", **options):\n",
    "    \"\"\"\n",
    "    Reads the demographics dataset indicated in the input_path, performs the ETL process and saves it in the output path indicated by the parameter out_put path.\n",
    "    \n",
    "    Args:\n",
    "        spark (:obj:`SparkSession`): Spark session. \n",
    "            Represents the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "        input_path (:obj:`str`): Directory where to find the input files.\n",
    "        output_path (:obj:`str`): Directory where to save immigration output files.\n",
    "        input_format (:obj:`str`): Type of the input files. Default to \"csv\" (comma-separated value).\n",
    "        columns (:obj:`list`): List of the columns names to read in. Useful when only some columns are useful.\n",
    "        load_size (int): Number of rows to read for debug purposes.\n",
    "        partitionBy (:obj:`list`): Files will be saved in partitions using the columns of this list.\n",
    "        header: (bool): Uses the first line as names of columns. If None is set, it uses the default value, false.\n",
    "        options: All other string options.\n",
    "    \"\"\"\n",
    "    # Loads the demographics dataframe using Spark\n",
    "    demographics = read_data(spark, input_path=input_path, input_format=input_format, \n",
    "                            columns=columns, debug_size = load_size, header=header, sep=sep, **options)\n",
    "    \n",
    "    # Convert numeric columns to the proper types: Integer and Double\n",
    "    int_cols = ['Count', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born']\n",
    "    float_cols = ['Median Age', 'Average Household Size']\n",
    "    demographics = cast_type(demographics, dict(zip(int_cols, len(int_cols)*[IntegerType()])))\n",
    "    demographics = cast_type(demographics, dict(zip(float_cols, len(float_cols)*[DoubleType()])))\n",
    "    \n",
    "    first_agg = {\"Median Age\": \"first\", \"Male Population\": \"first\", \"Female Population\": \"first\", \n",
    "                 \"Total Population\": \"first\", \"Number of Veterans\": \"first\", \"Foreign-born\": \"first\", \"Average Household Size\": \"first\"}\n",
    "    # First aggregation - City\n",
    "    agg_df = demographics.groupby([\"City\", \"State\", \"State Code\"]).agg(first_agg)\n",
    "    # Pivot Table to transform values of the column Race to different columns\n",
    "    piv_df = demographics.groupBy([\"City\", \"State\", \"State Code\"]).pivot(\"Race\").sum(\"Count\")\n",
    "    \n",
    "    # Rename column names removing the spaces to avoid problems when saving to disk (we got errors when trying to save column names with spaces)\n",
    "    demographics = agg_df.join(other=piv_df, on=[\"City\", \"State\", \"State Code\"], how=\"inner\")\\\n",
    "    .withColumnRenamed('first(Total Population)', 'TotalPopulation')\\\n",
    "    .withColumnRenamed('first(Female Population)', 'FemalePopulation')\\\n",
    "    .withColumnRenamed('first(Male Population)', 'MalePopulation')\\\n",
    "    .withColumnRenamed('first(Median Age)', 'MedianAge')\\\n",
    "    .withColumnRenamed('first(Number of Veterans)', 'NumberVeterans')\\\n",
    "    .withColumnRenamed('first(Foreign-born)', 'ForeignBorn')\\\n",
    "    .withColumnRenamed('first(Average Household Size)', 'AverageHouseholdSize')\\\n",
    "    .withColumnRenamed('Hispanic or Latino', 'HispanicOrLatino')\\\n",
    "    .withColumnRenamed('Black or African-American', 'BlackOrAfricanAmerican')\\\n",
    "    .withColumnRenamed('American Indian and Alaska Native', 'AmericanIndianAndAlaskaNative')\n",
    "    \n",
    "    numeric_cols = ['TotalPopulation', 'FemalePopulation', 'MedianAge', 'NumberVeterans', 'ForeignBorn', 'MalePopulation', 'AverageHouseholdSize',\n",
    "                    'AmericanIndianAndAlaskaNative', 'Asian', 'BlackOrAfricanAmerican', 'HispanicOrLatino', 'White']\n",
    "    # Fill the null values with 0\n",
    "    demographics = demographics.fillna(0, numeric_cols)\n",
    "    \n",
    "    # Save the demographics dataset to the output_path\n",
    "    if output_path is not None:\n",
    "        save(df=demographics, output_path=output_path, partitionBy = partitionBy)\n",
    "    \n",
    "    return demographics\n",
    "\n",
    "def etl_states_data(spark, output_path=\"out/state.parquet\"):\n",
    "    cols = ['TotalPopulation', 'FemalePopulation', 'MalePopulation', 'NumberVeterans', 'ForeignBorn', \n",
    "            'AmericanIndianAndAlaskaNative', 'Asian', 'BlackOrAfricanAmerican', 'HispanicOrLatino', 'White']\n",
    "    \"\"\"\n",
    "    Reads the states dataset indicated in the input_path, performs the ETL process and saves it in the output path indicated by the parameter out_put path.\n",
    "    \n",
    "    Args:\n",
    "        spark (:obj:`SparkSession`): Spark session. \n",
    "            Represents the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "        output_path (:obj:`str`): Directory where to save immigration output files.\n",
    "    \"\"\"\n",
    "    # Loads the demographics dataframe using Spark\n",
    "    demographics = etl_demographics_data(spark, output_path=None)\n",
    "    # Aggregates the dataset by State\n",
    "    states = demographics.groupby([\"State Code\", \"State\"]).agg(dict(zip(cols, len(cols)*[\"sum\"])))\n",
    "    # Loads the lookup table I94ADDR\n",
    "    addr = read_data(spark, input_path=\"lookup/I94ADDR.csv\", input_format=\"csv\", columns=\"*\", header=True)\\\n",
    "    .withColumnRenamed('State', 'State Original')\n",
    "    \n",
    "    # Join the two datasets\n",
    "    addr = addr.join(states, states[\"State Code\"] == addr.Code, \"left\")\n",
    "    addr = addr.withColumn(\"State\", when(isnull(addr[\"State\"]), capitalize_udf(addr['State Original'])).otherwise(addr[\"State\"]))\n",
    "    addr = addr.drop('State Original', 'State Code')\n",
    "    \n",
    "    cols = ['sum(BlackOrAfricanAmerican)', 'sum(White)', 'sum(AmericanIndianAndAlaskaNative)',\n",
    "            'sum(HispanicOrLatino)', 'sum(Asian)', 'sum(NumberVeterans)', 'sum(ForeignBorn)', 'sum(FemalePopulation)', \n",
    "            'sum(MalePopulation)', 'sum(TotalPopulation)']\n",
    "    \n",
    "    # Rename the columns to modify default names returned when Spark aggregates the values of the columns.\n",
    "    # For example: column 'sum(MalePopulation)' becomes 'MalePopulation'\n",
    "    mapping = dict(zip(cols, [re.search(r'\\((.*?)\\)', c).group(1) for c in cols]))\n",
    "    addr = rename_columns(addr, mapping)\n",
    "    \n",
    "    # Save the resulting dataset to the output_path\n",
    "    if output_path is not None:\n",
    "        save(df=addr, output_path=output_path)\n",
    "    return addr\n",
    "    \n",
    "def etl_countries_data(spark, input_path=\"../../data2/GlobalLandTemperaturesByCity.csv\", output_path=\"out/country.parquet\", \n",
    "                         input_format = \"csv\", columns = '*', load_size = None, header=True, **options):\n",
    "    \"\"\"\n",
    "    Reads the global temperatures dataset indicated in the input_path and transform it to generate the country dataframe. Performs the ETL process and saves it in the output path indicated by the parameter out_put path.\n",
    "    \n",
    "    Args:\n",
    "        spark (:obj:`SparkSession`): Spark session. \n",
    "            Represents the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "        input_path (:obj:`str`): Directory where to find the input files.\n",
    "        output_path (:obj:`str`): Directory where to save immigration output files.\n",
    "        input_format (:obj:`str`): Type of the input files. Default to \"csv\" (comma-separated value).\n",
    "        columns (:obj:`list`): List of the columns names to read in. Useful when only some columns are useful.\n",
    "        load_size (int): Number of rows to read for debug purposes.\n",
    "        header: (bool): Uses the first line as names of columns. If None is set, it uses the default value, false.\n",
    "        options: All other string options.\n",
    "    \"\"\"\n",
    "    # Loads the demographics dataframe using Spark\n",
    "    countries = read_data(spark, input_path=input_path, input_format=input_format, \n",
    "                            columns=columns, debug_size = load_size, header=header, **options)\n",
    "    # Aggregates the dataset by Country and rename the name of new columns\n",
    "    countries = countries.groupby([\"Country\"]).agg({\"AverageTemperature\": \"avg\", \"Latitude\": \"first\", \"Longitude\": \"first\"})\\\n",
    "    .withColumnRenamed('avg(AverageTemperature)', 'Temperature')\\\n",
    "    .withColumnRenamed('first(Latitude)', 'Latitude')\\\n",
    "    .withColumnRenamed('first(Longitude)', 'Longitude')\n",
    "    \n",
    "    # Rename specific country names to match the I94CIT_I94RES lookup table when joining them\n",
    "    change_countries = [(\"Country\", \"Congo (Democratic Republic Of The)\", \"Congo\"), (\"Country\", \"Côte D'Ivoire\", \"Ivory Coast\")]\n",
    "    countries = change_field_value_condition(countries, change_countries)\n",
    "    countries = countries.withColumn('Country_Lower', lower(countries.Country))\n",
    "    \n",
    "    # Rename specific country names to match the demographics dataset when joining them\n",
    "    change_res = [(\"I94CTRY\", \"BOSNIA-HERZEGOVINA\", \"BOSNIA AND HERZEGOVINA\"), \n",
    "                  (\"I94CTRY\", \"INVALID: CANADA\", \"CANADA\"),\n",
    "                  (\"I94CTRY\", \"CHINA, PRC\", \"CHINA\"),\n",
    "                  (\"I94CTRY\", \"GUINEA-BISSAU\", \"GUINEA BISSAU\"),\n",
    "                  (\"I94CTRY\", \"INVALID: PUERTO RICO\", \"PUERTO RICO\"),\n",
    "                  (\"I94CTRY\", \"INVALID: UNITED STATES\", \"UNITED STATES\")]\n",
    "    \n",
    "    # Loads the lookup table I94CIT_I94RES\n",
    "    res = read_data(spark, input_path=\"lookup/I94CIT_I94RES.csv\", input_format=input_format, columns=\"*\",\n",
    "                          debug_size = load_size, header=header, **options)\n",
    "    res = cast_type(res, {\"Code\": IntegerType()})\n",
    "    res = change_field_value_condition(res, change_res)\n",
    "    res = res.withColumn('Country_Lower', lower(res.I94CTRY))\n",
    "    # Join the two datasets to create the country dimmension table\n",
    "    res = res.join(countries, res.Country_Lower == countries.Country_Lower, how=\"left\")\n",
    "    res = res.withColumn(\"Country\", when(isnull(res[\"Country\"]), capitalize_udf(res.I94CTRY)).otherwise(res[\"Country\"]))   \n",
    "    res = res.drop(\"I94CTRY\", \"Country_Lower\")\n",
    "    \n",
    "    # Save the resulting dataset to the output_path\n",
    "    if output_path is not None:\n",
    "        save(df=res, output_path=output_path)\n",
    "    return res\n",
    "\n",
    "def cast_type(df, cols):\n",
    "    \"\"\"\n",
    "    Convert the types of the columns according to the configuration supplied in the cols dictionary in the format {\"column_name\": type}\n",
    "    \n",
    "    Args:\n",
    "        df (:obj:`SparkDataFrame`): Spark dataframe to be processed. \n",
    "            Represents the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "        cols (:obj:`dict`): Dictionary in the format of {\"column_name\": type} indicating what columns and types they should be converted to\n",
    "    \"\"\"\n",
    "    for k,v in cols.items():\n",
    "        if k in df.columns:\n",
    "            df = df.withColumn(k, df[k].cast(v))\n",
    "    return df\n",
    "\n",
    "def convert_sas_date(df, cols):\n",
    "    \"\"\"\n",
    "    Convert dates in the SAS datatype to a date in a string format YYYY-MM-DD\n",
    "    \n",
    "    Args:\n",
    "        df (:obj:`SparkDataFrame`): Spark dataframe to be processed. \n",
    "            Represents the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "        cols (:obj:`list`): List of columns in the SAS date format to be convert\n",
    "    \"\"\"\n",
    "    for c in [c for c in cols if c in df.columns]:\n",
    "        df = df.withColumn(c, convert_sas_udf(df[c]))\n",
    "    return df\n",
    "\n",
    "def change_field_value_condition(df, change_list):\n",
    "    '''\n",
    "    Helper function used to rename column values based on condition.\n",
    "    \n",
    "    Args:\n",
    "        df (:obj:`SparkDataFrame`): Spark dataframe to be processed.\n",
    "        change_list (:obj: `list`): List of tuples in the format (field, old value, new value)\n",
    "    '''\n",
    "    for field, old, new in change_list:\n",
    "        df = df.withColumn(field, when(df[field] == old, new).otherwise(df[field]))\n",
    "    return df\n",
    "\n",
    "def rename_columns(df, mapping):\n",
    "    '''\n",
    "    Rename the columns of the dataset based in the mapping dictionary\n",
    "    \n",
    "    Args:\n",
    "        df (:obj:`SparkDataFrame`): Spark dataframe to be processed.\n",
    "        mapping (:obj: `dict`): Mapping dictionary in the format {old_name: new_name}\n",
    "    '''\n",
    "    df = df.select([col(c).alias(mapping.get(c, c)) for c in df.columns])\n",
    "    return df\n",
    "\n",
    "def date_diff(date1, date2):\n",
    "    '''\n",
    "    Calculates the difference in days between two dates\n",
    "    '''\n",
    "    if date2 is None:\n",
    "        return None\n",
    "    else:\n",
    "        a = datetime.strptime(date1, date_format)\n",
    "        b = datetime.strptime(date2, date_format)\n",
    "        delta = b - a\n",
    "        return delta.days\n",
    "\n",
    "# User defined functions using Spark udf wrapper function to convert SAS dates into string dates in the format YYYY-MM-DD, to capitalize the first letters of the string and to calculate the difference between two dates in days.\n",
    "convert_sas_udf = udf(lambda x: x if x is None else (timedelta(days=x) + datetime(1960, 1, 1)).strftime(date_format))\n",
    "capitalize_udf = udf(lambda x: x if x is None else x.title())\n",
    "date_diff_udf = udf(date_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    This function creates a session with Spark, the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.config(\"spark.jars.packages\",\n",
    "                                        \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def read_data(spark, input_path, input_format = \"csv\", columns = '*', debug_size = None, **options):\n",
    "    \"\"\"\n",
    "    Loads data from a data source using the pyspark module and returns it as a spark 'DataFrame'.\n",
    "    \n",
    "    Args:\n",
    "        spark (:obj:`SparkSession`): Spark session. \n",
    "            Represents the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "        input_path (:obj:`str`): Directory where to find the input files.\n",
    "        input_format (:obj:`str`): Optional string for format of the data source. Default to 'csv'.\n",
    "        columns (:obj:`list`): List of columns of the dataframe to return. Default to \"*\", which means 'all columns'.\n",
    "        debug_size (int): Define the number of rows to read for debug purposes. The default value None means 'all rows'.\n",
    "        options: All other string options.\n",
    "    \"\"\"\n",
    "    if debug_size is None:\n",
    "        df = spark.read.load(input_path, format=input_format, **options).select(columns)\n",
    "    else:\n",
    "        df = spark.read.load(input_path, format=input_format, **options).select(columns).limit(debug_size)\n",
    "    return df\n",
    "\n",
    "def save(df, output_path, mode = \"overwrite\", output_format = \"parquet\", columns = '*', partitionBy=None, **options):\n",
    "    \"\"\"\n",
    "    Saves the contents of the DataFrame to a data source.\n",
    "    The data source is specified by the format and a set of options. If format is not specified, 'parquet' will be used.\n",
    "    \n",
    "    Args:\n",
    "        df (:obj:`DataFrame`): Spark DataFrame.\n",
    "        output_path (:obj:`str`): The path in a Hadoop supported file system where the DataFrame contentes will be saved.\n",
    "        mode (:obj:`str`): Specifies the behavior of the save operation when data already exists. Default to 'overwrite'.\n",
    "        output_format (:obj:`str`): Optional string for format of the data source to be saved. Default to 'parquet'.\n",
    "        columns (:obj:`list`): List of columns of the dataframe to save. Default to \"*\", which means 'all columns'.\n",
    "        partitionBy (:obj:`list`): Names of partitioning columns. The default value None means 'no partitions'.\n",
    "        options: All other string options.\n",
    "    \"\"\"\n",
    "\n",
    "    df.select(columns).write.save(output_path, mode= mode, format=output_format, partitionBy = partitionBy, **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def etl_immigration_data(spark, input_path=\"immigration_data_sample.csv\", output_path=\"out/immigration.parquet\",\n",
    "                         date_output_path=\"out/date.parquet\",\n",
    "                         input_format = \"csv\", columns = ['i94addr', 'i94mon','cicid','i94visa','i94res','arrdate','i94yr','depdate',\n",
    "                                                          'airline', 'fltno', 'i94mode', 'i94port', 'visatype', 'gender', \n",
    "                                                          'i94cit', 'i94bir'], \n",
    "                         load_size = None, partitionBy = [\"i94yr\", \"i94mon\"], columns_to_save='*', header=True, **options):\n",
    "    \"\"\"\n",
    "    Reads the immigration dataset indicated in the input_path, performs the ETL process and saves it in the output path indicated by the parameter \n",
    "    out_put path.\n",
    "    \n",
    "    Args:\n",
    "        spark (:obj:`SparkSession`): Spark session. \n",
    "            Represents the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "        input_path (:obj:`str`): Directory where to find the input files.\n",
    "        output_path (:obj:`str`): Directory where to save immigration output files.\n",
    "        date_output_path (:obj:`str`): Directory where to save date output files.\n",
    "        input_format (:obj:`str`): Type of the input files. Default to \"csv\" (comma-separated value).\n",
    "        columns (:obj:`list`): List of the columns names to read in. Useful when only some columns are useful.\n",
    "        load_size (int): Number of rows to read for debug purposes.\n",
    "        partitionBy (:obj:`list`): Files will be saved in partitions using the columns of this list.\n",
    "        columns_to_save (:obj:`list`): Define what columns will be saved.\n",
    "        header: (bool): Uses the first line as names of columns. If None is set, it uses the default value, false.\n",
    "        options: All other string options.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loads the immigration dataframe using Spark\n",
    "    # We discard the columns ['admnum', 'biryear', 'count', 'dtaddto', 'dtadfile', 'entdepa', 'entdepd', 'entdepu', 'insnum', 'matflag', 'occup', 'visapost'] as they seemed not to be very useful for our goals.\n",
    "    # Some of them were very unclear of what they really represent.\n",
    "    immigration = read_data(spark, input_path=input_path, input_format=input_format, \n",
    "                            columns=columns, debug_size = load_size, header=header, **options)\n",
    "    \n",
    "    int_cols = ['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', \n",
    "        'arrdate', 'i94mode', 'i94bir', 'i94visa', 'count', 'biryear', 'dtadfile', 'depdate']\n",
    "    \n",
    "    date_cols = ['arrdate', 'depdate']\n",
    "    \n",
    "    high_null = [\"visapost\", \"occup\", \"entdepu\", \"insnum\"]\n",
    "    not_useful_cols = [\"count\", \"entdepa\", \"entdepd\", \"matflag\", \"dtaddto\", \"biryear\", \"admnum\"]\n",
    "    \n",
    "    # Convert columns read as string/double to integer\n",
    "    immigration = cast_type(immigration, dict(zip(int_cols, len(int_cols)*[IntegerType()])))\n",
    "    \n",
    "    # Convert SAS date to a meaningful string date in the format of YYYY-MM-DD\n",
    "    immigration = convert_sas_date(immigration, date_cols)\n",
    "    \n",
    "    # Drop high null columns and not useful columns\n",
    "    immigration = immigration.drop(*high_null)\n",
    "    immigration = immigration.drop(*not_useful_cols)\n",
    "    \n",
    "    # Create a new columns to store the length of the visitor stay in the US\n",
    "    immigration = immigration.withColumn('stay', date_diff_udf(immigration.arrdate, immigration.depdate))\n",
    "    immigration = cast_type(immigration, {'stay': IntegerType()})\n",
    "    \n",
    "    # Generate DATE dataframe and save it to the output_path indicated as parameter of the function\n",
    "    if date_output_path is not None:\n",
    "        arrdate = immigration.select('arrdate').distinct()\n",
    "        depdate = immigration.select('depdate').distinct()\n",
    "        dates = arrdate.union(depdate)\n",
    "        dates = dates.withColumn(\"date\", to_date(dates.arrdate, date_format))\n",
    "        dates = dates.withColumn(\"year\", year(dates.date))\n",
    "        dates = dates.withColumn(\"month\", month(dates.date))\n",
    "        dates = dates.withColumn(\"day\", dayofmonth(dates.date))\n",
    "        dates = dates.withColumn(\"weekofyear\", weekofyear(dates.date))\n",
    "        dates = dates.withColumn(\"dayofweek\", dayofweek(dates.date))\n",
    "        dates = dates.drop(\"date\").withColumnRenamed('arrdate', 'date')\n",
    "        save(df=dates.select(\"date\", \"year\", \"month\", \"day\", \"weekofyear\", \"dayofweek\"), output_path=date_output_path)\n",
    "    \n",
    "    # Save the processed immigration dataset to the output_path\n",
    "    if output_path is not None:\n",
    "        save(df=immigration.select(columns_to_save), output_path=output_path, partitionBy = partitionBy)\n",
    "    return immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "  # Perform ETL process for the Immigration dataset generating immigration and date tables and save them in the S3 bucket indicated in the output_path parameters.\n",
    "immigration = etl_immigration_data(spark, input_path='../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat',\n",
    "                                     output_path=\"s3a://data--engineer-capstone/immigration.parquet\",\n",
    "                                     date_output_path=\"s3a://data-engineer-capstone/date.parquet\",\n",
    "                                     input_format = \"com.github.saurfang.sas.spark\", \n",
    "                                     load_size=1000, partitionBy=None, \n",
    "                                     columns_to_save = 'sparkprojdata')\n",
    "# Perform ETL process for the Country table. Generating the Country table and saving it in the S3 bucket indicated in the output_path parameter.\n",
    "countries = e.etl_countries_data(spark, output_path=e.OUTPUT + \"country.parquet\")\n",
    "# Perform ETL process for the State table. Generating the State table and saving it in the S3 bucket indicated in the output_path parameter.\n",
    "states = e.etl_states_data(spark, output_path=e.OUTPUT + \"state.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Countries dataset\n",
    "For country dataset it starts by loading the data global temperature dataset along with I94CIT_I94RES lookup table and is completed by generating and the storing of the processed dataframe to a bucket in Amazon S3.The following tasks are performed :\n",
    "* Loading of the csv file of the global temperature and I94CIT_I94RES lookup table;\n",
    "* Aggregation of the temperatures dataset by country and rename new columns;\n",
    "* Join the two datasets;\n",
    "* Save the resulting dataset to the staging area in Amazon S3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### States dataset\n",
    "The generation of the states dataset starts by loading the data in demographics dataset as well as I94ADDR lookup table and is completed by generating and the storing of the processed dataframe to a bucket in Amazon S3. In summary, the following tasks are performed throughout the process:\n",
    "* Loading of the csv file of the demographics and I94ADDR lookup table;\n",
    "* Aggregation of the demographics dataset by state and rename new columns;\n",
    "* Join the two datasets;\n",
    "* Save the resulting dataset to the staging area in Amazon S3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"images/etl_state.png\" alt=\"etl_state\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The parquet files are saved in the S3 bucket in the AWS and are used to load the tables of the same name in the Amazon Redshift. We create the schema by running the SQL script found in `sql/create_tables.sql`. From there, our model is ready to be explored by the customers whether through open query editor in Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "_Build the data pipelines to create the data model._\n",
    "\n",
    "Pipeline is divided into two stages. The first, where we used spark to load, extracted, transform and store the provided datasets into the AWS S3 staging area. The second stage is [Apache Airflow](https://airflow.apache.org/) to build a DAG to extract data from S3 and load them into tables of the same name in Amazon Redshift. As a final step we check the data counting checking to ensure completeness.\n",
    "\n",
    "<img src=\"images/architecture.png\" alt=\"architecture\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Below we show the pipeline of the second stage we developed using Apache Airflow.\n",
    "\n",
    "<img src=\"images/dag.PNG\" alt=\"dag\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The code to build the Airflow pipeline is located in the folder `airflow`. There you will find the code of the DAG itself (file `capstone.py` inside folder `dags`) as well as the two custom operators built for this capstone project in folder `plugins/operators`: `stage_redshift.py` and `data_quality.py`.\n",
    "\n",
    "The custom operator `StageToRedshiftOperator` was designed to load data in [parquet](https://parquet.apache.org/) format from S3 buckets in AWS and insert the content into a table in AWS Redshift. That operator is customizable to work with different buckets and with different tables by input parameters. Then it is used in our DAG to load to Redshift both fact and dimension tables.\n",
    "\n",
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "First, we load the `IMMIGRATION` fact table through the step `Immigration_Fact_Table`, which is followed by the steps to load the dimension tables `STATE`, `DATE` and `COUNTRY`, respectively `State_Dimension_Table`, `Date_Dimension_Table`, `Country_Dimension_Table` steps. All the tables have a primary key constraint that uniquely identify the records and in the fact table there are foriegn key that guarantee that values in the fact are present in the dimension tables.\n",
    "\n",
    "After completing the loading process, we perform a data quality check through the step `Data_Quality_Checks` to make sure everything was OK. In this check we verify if every table was actually loaded with count check in all the tables of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_immig_sample = pd.read_csv('immigration_data_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port',\n",
       "       'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa',\n",
       "       'count', 'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd',\n",
       "       'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum',\n",
       "       'airline', 'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immig_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>20568.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160423</td>\n",
       "      <td>MTR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20571.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160428</td>\n",
       "      <td>DOH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z</td>\n",
       "      <td>K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>721257</td>\n",
       "      <td>1481650.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20552.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GA</td>\n",
       "      <td>20606.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>10072016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DL</td>\n",
       "      <td>7.368526e+08</td>\n",
       "      <td>910</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1072780</td>\n",
       "      <td>2197173.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>SFR</td>\n",
       "      <td>20556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20635.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>10112016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CX</td>\n",
       "      <td>7.863122e+08</td>\n",
       "      <td>870</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>112205</td>\n",
       "      <td>232708.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20546.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20554.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>06302016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BA</td>\n",
       "      <td>5.547449e+10</td>\n",
       "      <td>00117</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2577162</td>\n",
       "      <td>5227851.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>20575.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>07262016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LX</td>\n",
       "      <td>5.941342e+10</td>\n",
       "      <td>00008</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10930</td>\n",
       "      <td>13213.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>06292016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>5.544979e+10</td>\n",
       "      <td>00109</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "5      721257  1481650.0  2016.0     4.0   577.0   577.0     ATL  20552.0   \n",
       "6     1072780  2197173.0  2016.0     4.0   245.0   245.0     SFR  20556.0   \n",
       "7      112205   232708.0  2016.0     4.0   113.0   135.0     NYC  20546.0   \n",
       "8     2577162  5227851.0  2016.0     4.0   131.0   131.0     CHI  20572.0   \n",
       "9       10930    13213.0  2016.0     4.0   116.0   116.0     LOS  20545.0   \n",
       "\n",
       "   i94mode i94addr  depdate  i94bir  i94visa  count  dtadfile visapost occup  \\\n",
       "0      1.0      HI  20573.0    61.0      2.0    1.0  20160422      NaN   NaN   \n",
       "1      1.0      TX  20568.0    26.0      2.0    1.0  20160423      MTR   NaN   \n",
       "2      1.0      FL  20571.0    76.0      2.0    1.0  20160407      NaN   NaN   \n",
       "3      1.0      CA  20581.0    25.0      2.0    1.0  20160428      DOH   NaN   \n",
       "4      3.0      NY  20553.0    19.0      2.0    1.0  20160406      NaN   NaN   \n",
       "5      1.0      GA  20606.0    51.0      2.0    1.0  20160408      NaN   NaN   \n",
       "6      1.0      CA  20635.0    48.0      2.0    1.0  20160412      NaN   NaN   \n",
       "7      1.0      NY  20554.0    33.0      2.0    1.0  20160402      NaN   NaN   \n",
       "8      1.0      IL  20575.0    39.0      2.0    1.0  20160428      NaN   NaN   \n",
       "9      1.0      CA  20553.0    35.0      2.0    1.0  20160401      NaN   NaN   \n",
       "\n",
       "  entdepa entdepd  entdepu matflag  biryear   dtaddto gender  insnum airline  \\\n",
       "0       G       O      NaN       M   1955.0  07202016      F     NaN      JL   \n",
       "1       G       R      NaN       M   1990.0  10222016      M     NaN     *GA   \n",
       "2       G       O      NaN       M   1940.0  07052016      M     NaN      LH   \n",
       "3       G       O      NaN       M   1991.0  10272016      M     NaN      QR   \n",
       "4       Z       K      NaN       M   1997.0  07042016      F     NaN     NaN   \n",
       "5       T       N      NaN       M   1965.0  10072016      M     NaN      DL   \n",
       "6       T       O      NaN       M   1968.0  10112016      F     NaN      CX   \n",
       "7       G       O      NaN       M   1983.0  06302016      F     NaN      BA   \n",
       "8       O       O      NaN       M   1977.0  07262016    NaN     NaN      LX   \n",
       "9       O       O      NaN       M   1981.0  06292016    NaN     NaN      AA   \n",
       "\n",
       "         admnum  fltno visatype  \n",
       "0  5.658267e+10  00782       WT  \n",
       "1  9.436200e+10  XBLNG       B2  \n",
       "2  5.578047e+10  00464       WT  \n",
       "3  9.478970e+10  00739       B2  \n",
       "4  4.232257e+10   LAND       WT  \n",
       "5  7.368526e+08    910       B2  \n",
       "6  7.863122e+08    870       B2  \n",
       "7  5.547449e+10  00117       WT  \n",
       "8  5.941342e+10  00008       WT  \n",
       "9  5.544979e+10  00109       WT  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "df_immig_sample.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "_Create a data dictionary for our data model. For each field, provided a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file._\n",
    "\n",
    "\n",
    "__Table Immigration__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| CICID | Primary Key |\n",
    "| I94YR | Year |\n",
    "| I94MON | Month |\n",
    "| I94CIT | 3 digit for the country code where the visitor was born. This is a FK to the COUNTRY dimension table |\n",
    "| I94RES | 3 digit for the country code where the visitor resides in. This is a FK to the COUNTRY dimension table |\n",
    "| ARRDATE | Arrival date in the USA. This is a FK to the DATE dimension table |\n",
    "| I94MODE | Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported) |\n",
    "| I94ADDR | State of arrival. This is a FK to the STATE dimension table |\n",
    "| DEPDATE | Departure date from the USA. This is a FK to the DATE dimension table |\n",
    "| I94BIR | Age of Respondent in Years |\n",
    "| I94VISA | Visa codes collapsed into three categories: (1 = Business; 2 = Pleasure; 3 = Student) |\n",
    "| BIRYEAR | 4 digit year of birth |\n",
    "| GENDER | Gender |\n",
    "| AIRLINE | Airline used to arrive in U.S. |\n",
    "| FLTNO | Flight number of Airline used to arrive in U.S. |\n",
    "| VISATYPE | Class of admission legally admitting the non-immigrant to temporarily stay in U.S. |\n",
    "| STAY | Number of days in the US |\n",
    "\n",
    "\n",
    "__Table STATE__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| Code | Primary Key. This is the code of the State as in I94ADDR lookup table |\n",
    "| State | Name of the state |\n",
    "| BlackOrAfricanAmerican | Number of residents of the race Black Or African American |\n",
    "| White | Number of residents of the race White |\n",
    "| ForeignBorn | Number of residents that born outside th United States |\n",
    "| AmericanIndianAndAlaskaNative | Number of residents of the race American Indian And Alaska Native |\n",
    "| HispanicOrLatino | Number of residents of the race Hispanic Or Latino |\n",
    "| Asian | Number of residents of the race Asian |\n",
    "| NumberVeterans | Number of residents that are war veterans |\n",
    "| FemalePopulation | Number of female population |\n",
    "| MalePopulation | Number of male population |\n",
    "| TotalPopulation | Number total of the population |\n",
    "\n",
    "\n",
    "__Table COUNTRY__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| Code | Country Code. This is the PK. |\n",
    "| Country | Country Name |\n",
    "| Temperature | Average temperature of the country between 1743 and 2013 |\n",
    "| Latitude | GPS Latitude |\n",
    "| Longitude | GPS Longitude |\n",
    "\n",
    "\n",
    "__Table DATE__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| date | Date in the format YYYY-MM-DD. This is the PK. |\n",
    "| day | Two digit day |\n",
    "| month | Two digit month |\n",
    "| year | Four digit for the year |\n",
    "| weekofyear | The week of the year |\n",
    "| dayofweek | The day of the week |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "__Clearly state the rationale for the choice of tools and technologies for the project.__\n",
    "\n",
    "The  solution is implemented on cloud computing technology, AWS in particular. Because  cloud computing provides a low-cost, scalable, and highly reliable infrastructure platform in the cloud this is a natural choice for every new solution like we did here. Every service we use (S3, EMR, Redshift) has reasonable cost and is ‘pay as you go’ pricing. So we can start small and scale as our solution grows. No up-front costs involved.\n",
    "\n",
    "In particular, why we use the following services:\n",
    "\n",
    "__S3:__ Provides a relatively cheap, easy-to-use with scalability, high availability, security, and performance. This seems to be perfect to a staging area like our solution here;\n",
    "__EMR:__ This is a cloud-native big data platform, allowing teams to process vast amounts of data quickly, and cost-effectively at scale using Spark. EMR is easy to use, secure, elastic and low-cost. Perfect to our project;\n",
    "\n",
    "_Redshift:__ A natural and logical choice since we based all the solution in the cloud in AWS. Redshift provides a massively parallel, column-oriented data warehouse that provides easy-scale functionality. The main analytical tools have native interface to load from Redshift.\n",
    "\n",
    "__Spark:__ This is simply the best framework for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Most of our team are pythonians and Spark has a very convenient API for python programmers to use;\n",
    "\n",
    "\n",
    "__Propose how often the data should be updated and why__\n",
    "\n",
    "Since we receive one file per month it seems reasonable to update the model monthly.\n",
    "\n",
    "__Write a description of how you would approach the problem differently under the following scenarios:__\n",
    "\n",
    " * The data was increased by 100x:\n",
    "\n",
    "Scaling the whole pipeline shouldn't be a problem. Since the solution is on Amazon cloud, that is easily scalable, the only thing we would need to do is the number of increase the number of nodes of the clusters in EMR should be increased to hadle more data. Also, Amazon Redshift is a data warehouse;\n",
    " \n",
    "* The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "The runnig interval of the Airflow DAG could be changed to daily and scheduled to run overnight to make the data available y 7am.\n",
    " \n",
    "* The database needed to be accessed by 100+ people.\n",
    " \n",
    "With Redshift we can make use of the feature \"elastic resize\" that enables us to add or remove nodes in an Amazon Redshift cluster in minutes. This further increases the agility to get better performance and more storage for demanding workloads, and to reduce cost during periods of low demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
